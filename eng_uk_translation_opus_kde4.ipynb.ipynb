{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation task (en-uk)\n",
    "\n",
    "Welcome to the 'eng_uk_translation_opus_kde4.ipynb' notebook! \n",
    "\n",
    "Here we will fine-tune the pretrained model for translation task (from English to Ukrainian) via Trainer API to achieve accurate translations, tailored to a specific domain.\n",
    "\n",
    "Mostly based on chapter from Hugging Face NLP Course: https://huggingface.co/learn/nlp-course/chapter7/4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sacrebleu # SacreBLEU, the most commonly used metric for benchmarking translation models (not used in most of other examples, so not present in requriements.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset kde4 (C:/Users/SUPERSOKOL/.cache/huggingface/datasets/kde4/en-uk-lang1=en,lang2=uk/0.0.0/243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac8f1140e7c4ab0ba1dd9a1ca83f633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "q:\\SANDBOX\\huggingface_nlp_tutorial\\.env\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "# all imports\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# dataset\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"uk\")\n",
    "\n",
    "# baseline model checkpoint\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-uk\"\n",
    "\n",
    "# baseline model\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\") \n",
    "\n",
    "# model to fine-tune\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# data collator (to deal with the padding)\n",
    "# we will use DataCollatorForSeq2Seq cause we need to pad not only inputs but also labels, and padding value for labels should be -100\n",
    "# also DataCollatorForSeq2Seq will be responsible for generating \"decoder_input_ids\" - shifted versions of the labels for our model \n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) # DataCollatorForSeq2Seq behaves differently depending on the model architecture so we need to pass model into it\n",
    "\n",
    "# define metrics\n",
    "metric = evaluate.load(\"sacrebleu\") # SacreBLEU metric, score can go from 0 to 100, and higher is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence in English: \n",
      "Open a module by clicking its name; a list of submodules will appear. Then, click one of the submodule category names to edit its configuration in the right pane.\n",
      "\n",
      "\n",
      "baseline translation: \n",
      "Відкрити модуль натисканням його назви; з' явиться список підмодулів. Після цього натисніть одну з назв підмодулів, щоб змінити налаштування підкатегорії на правій панелі.\n",
      "\n",
      "\n",
      "true translation: \n",
      "Якщо ви відкриєте модуль наведенням на його позначку вказівника миші з наступним клацанням лівою кнопкою миші, з’ явиться список підмодулів. Після цього вам слід натиснути назву одного підмодулів категорії, щоб отримати доступ до відповідних налаштувань на панелі праворуч.\n"
     ]
    }
   ],
   "source": [
    "# test baseline\n",
    "original_sentence = raw_datasets[\"train\"][6]['translation']['en']\n",
    "baseline_translation = translator(original_sentence)[0]['translation_text']\n",
    "true_translation = raw_datasets[\"train\"][6]['translation']['uk']\n",
    "print(f'original sentence in English: \\n{original_sentence}')\n",
    "print(f'\\n\\nbaseline translation: \\n{baseline_translation}')\n",
    "print(f'\\n\\ntrue translation: \\n{true_translation}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4033b28f40334b499827f0d6125d74c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/210249 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23bb50a907244708bc0e7513e19d5b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23362 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data preprocessing \n",
    "# split dataset\n",
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "# rename 'test' to 'validation'\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "\n",
    "max_length = 64    # set max length of sentence to 128 cause our sentences in dataset are pretty short\n",
    "\n",
    "# data preprocessing funcyion\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]       # english sentence\n",
    "    targets = [ex[\"uk\"] for ex in examples[\"translation\"]]      # ukrainian swentence\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, text_target=targets, max_length=max_length, truncation=True     # tokenize, note that we need to pass ukrainian sentence into 'text_targets' arg \n",
    "    )\n",
    "    return model_inputs\n",
    "\n",
    "# apply preprocessing\n",
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "# metrics computation function for model outputs\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds # get model prediction logits and true labels (tokenized)\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)         # decode model predictions\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)       # decode true labels\n",
    "\n",
    "    # Some simple post-processing (remove leading, and trailing whitespaces)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    # compute metric\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log in to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6afb6e7cf3724f2795d8beb47be1538a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login() # to log in to Hugging Face, so you’re able to upload your results to the Model Hub if you want to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final preparation before fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "q:\\SANDBOX\\huggingface_nlp_tutorial\\marian-finetuned-kde4-en-to-uk is already a clone of https://huggingface.co/SUPERSOKOL/marian-finetuned-kde4-en-to-uk. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "# to define our training args we will use Seq2SeqTrainingArguments, a subclass of TrainingArguments that contains a few more fields \n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"marian-finetuned-kde4-en-to-uk\",\n",
    "    evaluation_strategy=\"no\",       # we will just evaluate our model once before training and after cause evaluation takes a while\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16, # I use small batches cause my laptops' GPU is not very powerfull\n",
    "    per_device_eval_batch_size=32,  #\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,     # for evaluation during training\n",
    "    fp16=True,                      # speeds up training on modern GPUs, comment this line if you dont have GPU with CUDA\n",
    "    push_to_hub=True,               # to upload the model to the Hub at the end of each epoch, comment this line if you are not logged in to Hugging Face \n",
    ")\n",
    "\n",
    "# next we need to define trainer instance and pass everything to it\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,                                              # model to fine-tune\n",
    "    args,                                               # training args we defined earlier\n",
    "    train_dataset=tokenized_datasets[\"train\"],          # train split of preprocessed dataset\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],      # validation split of preprocessed dataset\n",
    "    data_collator=data_collator,                        # here we pass our DataCollatorForSeq2Seq instance \n",
    "    tokenizer=tokenizer,                                # our tokenizer instance\n",
    "    compute_metrics=compute_metrics,                    # our custom metrics function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c10356182764f10804629089e5ef2c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/731 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores before fine-tuning: \n",
      "{'eval_loss': 1.1544969081878662, 'eval_bleu': 48.76479032184241, 'eval_runtime': 1204.7683, 'eval_samples_per_second': 19.391, 'eval_steps_per_second': 0.607}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "q:\\SANDBOX\\huggingface_nlp_tutorial\\.env\\lib\\site-packages\\transformers\\optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcdad094deb4572bc2f61be7e2b60c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0977, 'learning_rate': 1.9747355604596302e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0133, 'learning_rate': 1.9493696573066485e-05, 'epoch': 0.08}\n",
      "{'loss': 0.9932, 'learning_rate': 1.9240037541536668e-05, 'epoch': 0.11}\n",
      "{'loss': 1.0227, 'learning_rate': 1.898637851000685e-05, 'epoch': 0.15}\n",
      "{'loss': 0.9757, 'learning_rate': 1.8732719478477033e-05, 'epoch': 0.19}\n",
      "{'loss': 0.9917, 'learning_rate': 1.8479060446947215e-05, 'epoch': 0.23}\n",
      "{'loss': 0.978, 'learning_rate': 1.8225401415417395e-05, 'epoch': 0.27}\n",
      "{'loss': 1.0071, 'learning_rate': 1.797174238388758e-05, 'epoch': 0.3}\n",
      "{'loss': 0.9657, 'learning_rate': 1.771859067042082e-05, 'epoch': 0.34}\n",
      "{'loss': 0.9652, 'learning_rate': 1.7464931638891007e-05, 'epoch': 0.38}\n",
      "{'loss': 0.9761, 'learning_rate': 1.7211272607361186e-05, 'epoch': 0.42}\n",
      "{'loss': 0.9672, 'learning_rate': 1.695761357583137e-05, 'epoch': 0.46}\n",
      "{'loss': 0.9703, 'learning_rate': 1.6704461862364612e-05, 'epoch': 0.49}\n",
      "{'loss': 0.9877, 'learning_rate': 1.6450802830834795e-05, 'epoch': 0.53}\n",
      "{'loss': 0.9607, 'learning_rate': 1.6197651117368035e-05, 'epoch': 0.57}\n",
      "{'loss': 0.9345, 'learning_rate': 1.5943992085838217e-05, 'epoch': 0.61}\n",
      "{'loss': 0.9341, 'learning_rate': 1.56903330543084e-05, 'epoch': 0.65}\n",
      "{'loss': 0.9665, 'learning_rate': 1.5436674022778582e-05, 'epoch': 0.68}\n",
      "{'loss': 0.9445, 'learning_rate': 1.5183522309311824e-05, 'epoch': 0.72}\n",
      "{'loss': 0.9546, 'learning_rate': 1.4929863277782007e-05, 'epoch': 0.76}\n",
      "{'loss': 0.9543, 'learning_rate': 1.4676204246252188e-05, 'epoch': 0.8}\n",
      "{'loss': 0.9454, 'learning_rate': 1.4422545214722372e-05, 'epoch': 0.84}\n",
      "{'loss': 0.9397, 'learning_rate': 1.4168886183192555e-05, 'epoch': 0.88}\n",
      "{'loss': 0.9354, 'learning_rate': 1.3915227151662736e-05, 'epoch': 0.91}\n",
      "{'loss': 0.9597, 'learning_rate': 1.3661568120132918e-05, 'epoch': 0.95}\n",
      "{'loss': 0.9418, 'learning_rate': 1.340841640666616e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding files tracked by Git LFS: ['source.spm', 'target.spm']. This may take a bit of time if the files are large.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8478, 'learning_rate': 1.3154757375136343e-05, 'epoch': 1.03}\n",
      "{'loss': 0.831, 'learning_rate': 1.2901098343606527e-05, 'epoch': 1.07}\n",
      "{'loss': 0.8111, 'learning_rate': 1.2647439312076708e-05, 'epoch': 1.1}\n",
      "{'loss': 0.804, 'learning_rate': 1.239378028054689e-05, 'epoch': 1.14}\n",
      "{'loss': 0.8177, 'learning_rate': 1.2140121249017071e-05, 'epoch': 1.18}\n",
      "{'loss': 0.8121, 'learning_rate': 1.1886462217487256e-05, 'epoch': 1.22}\n",
      "{'loss': 0.833, 'learning_rate': 1.1632803185957437e-05, 'epoch': 1.26}\n",
      "{'loss': 0.8165, 'learning_rate': 1.1379651472490678e-05, 'epoch': 1.29}\n",
      "{'loss': 0.8338, 'learning_rate': 1.1125992440960863e-05, 'epoch': 1.33}\n",
      "{'loss': 0.8149, 'learning_rate': 1.0872333409431044e-05, 'epoch': 1.37}\n",
      "{'loss': 0.8045, 'learning_rate': 1.0618674377901226e-05, 'epoch': 1.41}\n",
      "{'loss': 0.8201, 'learning_rate': 1.0365522664434468e-05, 'epoch': 1.45}\n",
      "{'loss': 0.8127, 'learning_rate': 1.011186363290465e-05, 'epoch': 1.48}\n",
      "{'loss': 0.8191, 'learning_rate': 9.858204601374832e-06, 'epoch': 1.52}\n",
      "{'loss': 0.8214, 'learning_rate': 9.604545569845016e-06, 'epoch': 1.56}\n",
      "{'loss': 0.7968, 'learning_rate': 9.351393856378257e-06, 'epoch': 1.6}\n",
      "{'loss': 0.8126, 'learning_rate': 9.09773482484844e-06, 'epoch': 1.64}\n",
      "{'loss': 0.824, 'learning_rate': 8.844075793318621e-06, 'epoch': 1.67}\n",
      "{'loss': 0.8054, 'learning_rate': 8.590924079851863e-06, 'epoch': 1.71}\n",
      "{'loss': 0.8186, 'learning_rate': 8.337265048322047e-06, 'epoch': 1.75}\n",
      "{'loss': 0.8095, 'learning_rate': 8.083606016792228e-06, 'epoch': 1.79}\n",
      "{'loss': 0.8291, 'learning_rate': 7.82994698526241e-06, 'epoch': 1.83}\n",
      "{'loss': 0.8125, 'learning_rate': 7.5762879537325925e-06, 'epoch': 1.86}\n",
      "{'loss': 0.8024, 'learning_rate': 7.322628922202776e-06, 'epoch': 1.9}\n",
      "{'loss': 0.7974, 'learning_rate': 7.0694772087360176e-06, 'epoch': 1.94}\n",
      "{'loss': 0.8021, 'learning_rate': 6.8158181772062e-06, 'epoch': 1.98}\n",
      "{'loss': 0.7719, 'learning_rate': 6.562159145676382e-06, 'epoch': 2.02}\n",
      "{'loss': 0.7096, 'learning_rate': 6.3085001141465655e-06, 'epoch': 2.05}\n",
      "{'loss': 0.7273, 'learning_rate': 6.054841082616747e-06, 'epoch': 2.09}\n",
      "{'loss': 0.7237, 'learning_rate': 5.80118205108693e-06, 'epoch': 2.13}\n",
      "{'loss': 0.7151, 'learning_rate': 5.547523019557112e-06, 'epoch': 2.17}\n",
      "{'loss': 0.7335, 'learning_rate': 5.2938639880272944e-06, 'epoch': 2.21}\n",
      "{'loss': 0.7324, 'learning_rate': 5.040712274560536e-06, 'epoch': 2.24}\n",
      "{'loss': 0.7241, 'learning_rate': 4.787053243030719e-06, 'epoch': 2.28}\n",
      "{'loss': 0.7305, 'learning_rate': 4.533394211500901e-06, 'epoch': 2.32}\n",
      "{'loss': 0.7371, 'learning_rate': 4.279735179971083e-06, 'epoch': 2.36}\n",
      "{'loss': 0.7269, 'learning_rate': 4.026583466504326e-06, 'epoch': 2.4}\n",
      "{'loss': 0.7265, 'learning_rate': 3.7729244349745074e-06, 'epoch': 2.44}\n",
      "{'loss': 0.7331, 'learning_rate': 3.51977272150775e-06, 'epoch': 2.47}\n",
      "{'loss': 0.7312, 'learning_rate': 3.2661136899779317e-06, 'epoch': 2.51}\n",
      "{'loss': 0.706, 'learning_rate': 3.012454658448114e-06, 'epoch': 2.55}\n",
      "{'loss': 0.7365, 'learning_rate': 2.7587956269182966e-06, 'epoch': 2.59}\n",
      "{'loss': 0.7163, 'learning_rate': 2.5051365953884788e-06, 'epoch': 2.63}\n",
      "{'loss': 0.7268, 'learning_rate': 2.2514775638586614e-06, 'epoch': 2.66}\n",
      "{'loss': 0.7223, 'learning_rate': 1.9978185323288437e-06, 'epoch': 2.7}\n",
      "{'loss': 0.7233, 'learning_rate': 1.744159500799026e-06, 'epoch': 2.74}\n",
      "{'loss': 0.7255, 'learning_rate': 1.4910077873322681e-06, 'epoch': 2.78}\n",
      "{'loss': 0.7094, 'learning_rate': 1.2373487558024506e-06, 'epoch': 2.82}\n",
      "{'loss': 0.7305, 'learning_rate': 9.836897242726328e-07, 'epoch': 2.85}\n",
      "{'loss': 0.7262, 'learning_rate': 7.300306927428151e-07, 'epoch': 2.89}\n",
      "{'loss': 0.733, 'learning_rate': 4.763716612129975e-07, 'epoch': 2.93}\n",
      "{'loss': 0.7218, 'learning_rate': 2.2321994774623952e-07, 'epoch': 2.97}\n",
      "{'train_runtime': 5385.3199, 'train_samples_per_second': 117.123, 'train_steps_per_second': 7.32, 'train_loss': 0.837136649693558, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa48187767604d11887335ebfcfcde9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/731 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "scores after fine-tuning: \n",
      "{'eval_loss': 0.7623581290245056, 'eval_bleu': 50.09005982889118, 'eval_runtime': 1322.1733, 'eval_samples_per_second': 17.669, 'eval_steps_per_second': 0.553, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095a714a32874863b2d3893c5cd3798c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Aug17_14-46-59_DESKTOP-SH88-S-K/events.out.tfevents.1692274030.DESKTOP-SH88-S-K.38808.0: 100%…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fcc7295b524455289fcbb697152aad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Aug17_14-46-59_DESKTOP-SH88-S-K/events.out.tfevents.1692280738.DESKTOP-SH88-S-K.38808.1: 100%…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/SUPERSOKOL/marian-finetuned-kde4-en-to-uk\n",
      "   6c54a97..e38e2d0  main -> main\n",
      "\n",
      "To https://huggingface.co/SUPERSOKOL/marian-finetuned-kde4-en-to-uk\n",
      "   e38e2d0..c84c9e7  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/SUPERSOKOL/marian-finetuned-kde4-en-to-uk/commit/e38e2d0d88dda090f8a918565ea7950c3d93f242'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"     # split size set to 512mb to avoid CUDA \"out of memory\" error \n",
    "\n",
    "# take a look at score our model gets before fine-tuning (will take a bit of time)\n",
    "start_scores = trainer.evaluate(max_length=max_length)\n",
    "print(f'scores before fine-tuning: \\n{start_scores}')\n",
    "\n",
    "# train model (will take a bit of time)\n",
    "trainer.train()\n",
    "\n",
    "# after training we need to evaluate our model again (will take a bit of time)\n",
    "final_scores = trainer.evaluate(max_length=max_length)\n",
    "print(f'\\n\\nscores after fine-tuning: \\n{final_scores}')\n",
    "\n",
    "# push our model to Model Hub, comment this line if you are not logged in to Hugging Face \n",
    "trainer.push_to_hub(tags=\"translation\", commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence in English: \n",
      "Open a module by clicking its name; a list of submodules will appear. Then, click one of the submodule category names to edit its configuration in the right pane.\n",
      "\n",
      "\n",
      "baseline translation: \n",
      "Відкрити модуль натисканням його назви; з' явиться список підмодулів. Після цього натисніть одну з назв підмодулів, щоб змінити налаштування підкатегорії на правій панелі.\n",
      "\n",
      "\n",
      "true translation: \n",
      "Якщо ви відкриєте модуль наведенням на його позначку вказівника миші з наступним клацанням лівою кнопкою миші, з’ явиться список підмодулів. Після цього вам слід натиснути назву одного підмодулів категорії, щоб отримати доступ до відповідних налаштувань на панелі праворуч.\n",
      "\n",
      "\n",
      "fine-tuned model translation: \n",
      "Відкрити модуль можна натисканням його назви. Буде відкрито список підмодулів. Після цього натисніть одну з підмодулів назв категорій, щоб змінити налаштування підкатегорії на правій панелі.\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuned model loading and usage (if you have pushed your model to Hub)\n",
    "model_checkpoint = \"SUPERSOKOL/marian-finetuned-kde4-en-to-uk\"                      # Replace this with your own checkpoint \n",
    "translator_fine_tuned = pipeline(\"translation\", model=model_checkpoint)             # Load our fine-tuned model from Hub\n",
    "new_translation = translator_fine_tuned(original_sentence)[0]['translation_text']   # Feed sentence into pipeline     \n",
    "\n",
    "# compare our model to baseline\n",
    "\n",
    "print(f'original sentence in English: \\n{original_sentence}')       # original sentence\n",
    "print(f'\\n\\nbaseline translation: \\n{baseline_translation}')        # baseline translation\n",
    "print(f'\\n\\ntrue translation: \\n{true_translation}')                # ground truth\n",
    "\n",
    "print(f'\\n\\nfine-tuned model translation: \\n{new_translation}')             # fine-tuned model translation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
